\section{深度学习技巧总结}

为改善深度学习的效果，针对不同情况需要采取不同措施。在模型发生欠拟合和过拟合时采取的方法是不同的，结合机器学习三步骤，改善模型的选择流程如图\ref{fig:recipe_of_dl}所示。
\begin{figure}[htb]
	\centering
	\includegraphics[scale=0.4]{pic/recipe_of_dl}
	\caption{recipe of deep learning}
	\label{fig:recipe_of_dl}
\end{figure}
因此在模型在测试集上取得比较差的结果时，必须确定是在训练阶段结果就差，还是模型发生了过拟合仅在测试集上效果较差，依次才能针对不同情况加以改善。如图\ref{fig:blame_overfitting}所示，在测试集上，更为复杂的56层网络结构的效果不如20层网络，直觉上发生了过拟合，但实际在训练阶段，56层的模型就没有训练好。因此，应该修正模型的训练过程，再在测试集上测试。
\begin{figure}[b]
	\centering
	\includegraphics[scale=0.4]{pic/blame_overfitting}
	\caption{do not always blame overfitting}
	\label{fig:blame_overfitting}
\end{figure}
\begin{myquotation}{tips for overfitting and underfitting}
	
\begin{itemize}
	\item underfitting
		\subitem	选择新的激活函数
		\subitem 	自适应学习率
	\item overfitting
		\subitem	及早停止
		\subitem	正则化
		\subitem	dropout 
\end{itemize}
	
\end{myquotation}

\subsection{underfitting}

\subsubsection{梯度消失}

实际上，随着网络结构的复杂，在训练集上并不是更能够取得更优的结果，存在的主要问题就是梯度消失问题，简单的说就是，在反向传播时，靠近输入层的权重不容易得到更新，相比而言，靠近输出层的权重能够得到大的更新，如果开始阶段，权重是通过随机初始化的，那么靠近输入层的权重没有经过太大更新，还保持随机的状态，对整个网络而言，达到一个局部最优点是权重就停止更新了。

对 sigmoid 激活函数而言$\sigma ' (z) = \sigma(z)(1-\sigma(z))$,在$z=0$是取得最大值0.25。现在，如果使用标准方法来初始化网络中的权重，那么会使用一个均值为0，标准差为1的高斯分布。因此所有的权重通常会满足$|wj|<1$。从而有$\sigma'(z)w < 0.25$,前面的层上的梯度是来自后面的层上项的乘积,层数越多，靠近输入层的权重更新的就越小，这就是消失的梯度出现的本质原因。

因此，解决梯度消失问题的一个方法就是使用新的激活函数。

\subsubsection{ReLU}
\begin{equation}
\sigma(z)=\begin{cases}
0,\mathrm{if}\quad z \leq 0\\
z,\mathrm{if} \quad z > 0
\end{cases}
\end{equation}
从某种角度上看，当$z\leq 0$时，该单元没有激活，另外，在$z >0 $时,$\sigma'(z)=1$，解决了梯度消失问题。
\subsubsection{ReLU variant}
ReLU有几种变种，如leaky ReLU和parametric ReLU。区别是前者在$z\leq 0$时，取$\sigma(z)=0.01z$,后者的斜率是和梯度相关的，而非定值。

\subsubsection{maxout}
maxout 方法的主要思路是激活函数也是通过学习得到的，将网络中的单元分组，如两个一组，最终的输出结果为最大的值，思想和maxpooling 类似，示意图如图\ref{fig:maxout}所示。
\begin{figure}[htb]
	\centering
	\includegraphics[scale=0.4]{pic/maxout}
	\caption{maxout 示意图}
	\label{fig:maxout}
\end{figure}

实质上，ReLU函数可以看做是 maxout 在特定情况下的特例。如图\ref{fig:relu_isa_maxout}所示：
\begin{figure}[htb]
	\centering
	\includegraphics[scale=0.4]{pic/relu_isa_maxout}
	\caption{relu is a maxout}
	\label{fig:relu_isa_maxout}
\end{figure}

一般情况下，可以学习出不同类型的激活函数，激活函数的线性形状与权值有关，如图\ref{fig:learnable_activation}所示：

\begin{figure}[htb]
	\centering
	\includegraphics[scale=0.4]{pic/learnable_activation}
	\caption{learnable activation}
	\label{fig:learnable_activation}
\end{figure}

激活函数的线性分段数与单元的分组情况有关，如两段和三段的情况如图\ref{fig:pieces_of_activation}所示：
\begin{figure}[htb]
	\centering
	\includegraphics[scale=0.4]{pic/pieces_of_activation}
	\caption{pieces of activation}
	\label{fig:pieces_of_activation}
\end{figure}

实质上，maxout 方法也如同ReLU 函数，每次仅有有限的单元被激活，也只有被激活的单元权重会被更新，但不总是仅有某些单元一直失活，随着输入数据的不同，不同的单元会失活。

\subsubsection{自适应学习率}
adagrad:使用所有迭代的一阶导数来近似二阶导数
\[
w ^ { t + 1 } \leftarrow w ^ { t } - \frac { \eta } { \sqrt { \sum _ { i = 0 } ^ { t } \left( g ^ { i } \right) ^ { 2 } } } g ^ { t }
\]

RMSprop:
\begin{align}
w ^ { 1 } \leftarrow w ^ { 0 } - \frac { \eta } { \sigma ^ { 0 } } g ^ { 0 } \qquad & \sigma ^ { 0 } = g ^ { 0 }\\
w ^ { 2 } \leftarrow w ^ { 1 } - \frac { \eta } { \sigma ^ { 1 } } g ^ { 1 } \qquad & \sigma ^ { 1 } = \sqrt { \alpha \left( \sigma ^ { 0 } \right) ^ { 2 } + ( 1 - \alpha ) \left( g ^ { 1 } \right) ^ { 2 } }\\
w ^ { t + 1 } \leftarrow w ^ { t } - \frac { \eta } { \sigma ^ { t } } g ^ { t } \qquad & \sigma ^ { t } = \sqrt { \alpha \left( \sigma ^ { t - 1 } \right) ^ { 2 } + ( 1 - \alpha ) \left( g ^ { t } \right) ^ { 2 } }
\end{align}

未解决在局部最优点、saddle point、plateau等导数为零或近似于零的地方权重停止更新，从物理领域进入了动量的概念，解决此问题。

Momentum：
与经典的梯度下降法中权重的更新方式，momentum中也每一步前的权重更新的方向考虑在内，实际上是以前所有的梯度都考虑在内，只不过越远的权重的比例越小。对比如图\ref{fig:gd_vs_monentum}所示。
\begin{figure}[htb]
	\centering
	
	{\subcaptionbox{gradient decent}{%
			\includegraphics[scale=0.4]{pic/gd_plot}}\quad
		\subcaptionbox{momentum}{%
			\includegraphics[scale=0.4]{pic/momentum_plot}}
	}
	\caption{经典梯度权重更新与momentum 方法}
	\label{fig:gd_vs_monentum}
\end{figure}

momentum 中每一步权重增量公式如图\ref{fig:momentum_weight}所示。
\begin{figure}[htb]
	\centering
	\includegraphics[scale=0.4]{pic/momentum_weight}
	\caption{momentum weight increment}
	\label{fig:momentum_weight}
\end{figure}

Adam:将adagrad与RMSprop结合，具体如图\ref{fig:adam}。

\begin{figure}[htb]
	\centering
	\includegraphics[scale=0.6]{pic/adam}
	\caption{adam 方法}
	\label{fig:adam}
\end{figure}

\subsection{overfitting}

\subsubsection{及早停止}
顾名思义，过拟合时在测试集上的正确率不减反增，所以选择合适的epoch停止网络训练，当然，这里的测试集指的是验证集。
\subsubsection{正则化}
$l_2$正则化：$\alpha \|w\|^2_2=\sum_i w_i^2$

\[
L'(w)=L(w) + \frac{1}{2}\alpha \|w\|^2_2  
\]
\[
\frac{\partial L'(w)}{w_i}=\frac{\partial L(w)}{w_i} + \alpha w_i
\]

\[
w^{t+1} = w^{t} - \eta (\frac{\partial L}{w} + \alpha w^t)=(1-\eta \alpha)w^t - \eta \frac{\partial L}{w}
\]
随着权重的迭代更新，$(1-\eta \alpha)$越来越小，逐渐接近于零。
$l_1$正则化：$\alpha \|w\|_1 = \sum_i |w_i|$

\[
L'(w)=L(w) + \alpha \|w\|_1 
\]
\[
\frac{\partial L'(w)}{w_i}=\frac{\partial L(w)}{w_i} + \alpha sgn(w_i)
\]

\[
w^{t+1} = w^{t} - \eta (\frac{\partial L(w)}{w} + \alpha sgn(w^t))=w^t-\eta \frac{\partial L(w)}{w} -\eta \alpha sgn(w^t)
\]

在两种情形下，规范化的效果就是缩小权重。这符合我们的直觉，两种规范化都惩罚大的权重。
但权重缩小的方式不同\footnote{电子书：Michael Nielsen，神经网络与深度学习}。在 L1 规范化中，权重通过一个常量$\eta \alpha sgn(w^t)$向 0 进行缩小。在 L2 规范化中，权
重通过一个和 $w$ 成比例的量$(1-\eta \alpha)$进行缩小的。所以，当一个特定的权重绝对值 $|w|$ 很大时， L1 规范
化的权重缩小得远比 L2 规范化要小得多。相反，当一个特定的权重绝对值 $|w|$ 很小时， L1 规范
化的权重缩小得要比 L2 规范化大得多。最终的结果就是： L1 规范化倾向于聚集网络的权重在
相对少量的高重要度连接上，而其他权重就会被驱使向 0 接近。

一种形象的解释见图\ref{fig:regularization}：
\begin{figure}[htb]
	\centering
	
	{\subcaptionbox{l1 regularization}{%
			\includegraphics[scale=0.4]{pic/regularization1}}\quad
		\subcaptionbox{l2 regularization}{%
			\includegraphics[scale=0.4]{pic/regularization2}}
	}
	\caption{l1 vs l2 regularization}
	\label{fig:regularization}
\end{figure}
其它更为理论解释见知乎《深入理解L1、L2正则化》\footnote{\url{https://zhuanlan.zhihu.com/p/29360425}}。
\subsubsection{dropout}
与经典神经网络结构有所区别，在每一批量训练数据时，即前向传播和反向传播阶段，除了输入层和输出层的单元，其它单元有一定比率$p\%$失活。因此，对每一批训练数据得到的模型是不同的，训练结束后，对于测试数据使用的模型，所有权重均有保留，但权重取最终值的$1-p\%$。可以与过拟合时的图形解释见图\ref{fig:bias_and_variance}，每次得到的模型都是复杂的，方差大，但大量模型求取均值后就与实际模型基本一致，而每一批次数据使用dropout 得到的模型时不同的，因此提供了大量模型，对测试集数据使用的就是求取均值后的结果，因此对过拟合问题有所帮助。